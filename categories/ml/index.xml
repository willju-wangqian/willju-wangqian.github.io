<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML on Wangqian Ju&#39;s website</title>
    <link>/categories/ml/</link>
    <description>Recent content in ML on Wangqian Ju&#39;s website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Apr 2022 14:56:40 -0500</lastBuildDate><atom:link href="/categories/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ML Course Review for Neural Networks</title>
      <link>/2022/04/27/ml-course-review-for-neural-networks/</link>
      <pubDate>Wed, 27 Apr 2022 14:56:40 -0500</pubDate>
      
      <guid>/2022/04/27/ml-course-review-for-neural-networks/</guid>
      <description>deal with non-linear
 explicitly project features to higher dimensional space use svm with kernel function (implecitly)  use tanh(x) instead of sign(x) to enable gradient descent
patterns in back propagation
 add: gradient distributor max: gradient router mul: gradient switcher  border pixels are less used without padding
receptive field
 the region of the input sapce that affects a particular unit of the network  pooling
 can reduce the dimension of the feature maps and save computations reduction operation like max and average number of trainable parameters is 0 enlarge receptive fields  why ReLU is popular</description>
    </item>
    
  </channel>
</rss>
